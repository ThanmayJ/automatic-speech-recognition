{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "pdtransformer_asr",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "cf28b1cffad21296a8361ae6bbc573579d2c3478f41b644f350ce7a52ddb2396"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "import os\r\n",
        "from glob import glob\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import pandas as pd\r\n",
        "from jiwer import wer"
      ],
      "outputs": [],
      "metadata": {
        "id": "UqG7XZJIiA_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kgWaCoXAfaN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View the dataset"
      ],
      "metadata": {
        "id": "H8Mvod7CiA_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "directory = \".\\datasets\\LJSpeech-1.1\"\r\n",
        "wavs = glob(\"{}/**/*.wav\".format(directory), recursive=True)\r\n",
        "\r\n",
        "df = pd.read_csv(os.path.join(directory, \"metadata.csv\"), encoding=\"utf-8\", sep='|', header = None)\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJ001-0001</td>\n",
              "      <td>Printing, in the only sense with which we are ...</td>\n",
              "      <td>Printing, in the only sense with which we are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJ001-0002</td>\n",
              "      <td>in being comparatively modern.</td>\n",
              "      <td>in being comparatively modern.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJ001-0003</td>\n",
              "      <td>For although the Chinese took impressions from...</td>\n",
              "      <td>For although the Chinese took impressions from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LJ001-0004</td>\n",
              "      <td>produced the block books, which were the immed...</td>\n",
              "      <td>produced the block books, which were the immed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LJ001-0005</td>\n",
              "      <td>the invention of movable metal letters in the ...</td>\n",
              "      <td>the invention of movable metal letters in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13095</th>\n",
              "      <td>LJ050-0274</td>\n",
              "      <td>made certain recommendations which it believes...</td>\n",
              "      <td>made certain recommendations which it believes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13096</th>\n",
              "      <td>LJ050-0275</td>\n",
              "      <td>materially improve upon the procedures in effe...</td>\n",
              "      <td>materially improve upon the procedures in effe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13097</th>\n",
              "      <td>LJ050-0276</td>\n",
              "      <td>As has been pointed out, the Commission has no...</td>\n",
              "      <td>As has been pointed out, the Commission has no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13098</th>\n",
              "      <td>LJ050-0277</td>\n",
              "      <td>with the active cooperation of the responsible...</td>\n",
              "      <td>with the active cooperation of the responsible...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13099</th>\n",
              "      <td>LJ050-0278</td>\n",
              "      <td>the recommendations we have here suggested wou...</td>\n",
              "      <td>the recommendations we have here suggested wou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13100 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0                                                  1  \\\n",
              "0      LJ001-0001  Printing, in the only sense with which we are ...   \n",
              "1      LJ001-0002                     in being comparatively modern.   \n",
              "2      LJ001-0003  For although the Chinese took impressions from...   \n",
              "3      LJ001-0004  produced the block books, which were the immed...   \n",
              "4      LJ001-0005  the invention of movable metal letters in the ...   \n",
              "...           ...                                                ...   \n",
              "13095  LJ050-0274  made certain recommendations which it believes...   \n",
              "13096  LJ050-0275  materially improve upon the procedures in effe...   \n",
              "13097  LJ050-0276  As has been pointed out, the Commission has no...   \n",
              "13098  LJ050-0277  with the active cooperation of the responsible...   \n",
              "13099  LJ050-0278  the recommendations we have here suggested wou...   \n",
              "\n",
              "                                                       2  \n",
              "0      Printing, in the only sense with which we are ...  \n",
              "1                         in being comparatively modern.  \n",
              "2      For although the Chinese took impressions from...  \n",
              "3      produced the block books, which were the immed...  \n",
              "4      the invention of movable metal letters in the ...  \n",
              "...                                                  ...  \n",
              "13095  made certain recommendations which it believes...  \n",
              "13096  materially improve upon the procedures in effe...  \n",
              "13097  As has been pointed out, the Commission has no...  \n",
              "13098  with the active cooperation of the responsible...  \n",
              "13099  the recommendations we have here suggested wou...  \n",
              "\n",
              "[13100 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "id": "DNTv_afLYS0w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d97fb572-fd77-4b73-8718-f7acf7d10b48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Creating index-to-text dictionary\r\n",
        "\r\n",
        "id_to_text = {}\r\n",
        "with open(os.path.join(directory, \"metadata.csv\"), encoding=\"utf-8\") as f:\r\n",
        "    for line in f:\r\n",
        "        id = line.strip().split(\"|\")[0]  # Contains the ID of the speech sample\r\n",
        "        text = line.strip().split(\"|\")[2]  # Contains the transcript of the corresponding ID\r\n",
        "        id_to_text[id] = text                # Insert to dictionary\r\n",
        "\r\n",
        "\r\n",
        "def get_data(wavs, id_to_text, maxlen=50):\r\n",
        "    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\r\n",
        "    data = []\r\n",
        "    for w in wavs:\r\n",
        "        id = w.split(\"\\\\\")[-1].split(\".\")[0] # get the ID from the wav file name\r\n",
        "        if len(id_to_text[id]) < maxlen:\r\n",
        "            data.append({\"audio\": w, \"text\": id_to_text[id]})\r\n",
        "    return data"
      ],
      "outputs": [],
      "metadata": {
        "id": "N2nefAPl-wFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the dataset"
      ],
      "metadata": {
        "id": "mT2NatBgiA_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class VectorizeChar:\r\n",
        "    def __init__(self, max_len=50):\r\n",
        "        self.vocab = (\r\n",
        "            [\"-\", \"#\", \"<\", \">\"]\r\n",
        "            + [chr(i + 96) for i in range(1, 27)]\r\n",
        "            + [\" \", \".\", \",\", \"?\"]\r\n",
        "        )\r\n",
        "        self.max_len = max_len\r\n",
        "        self.char_to_idx = {}\r\n",
        "        for i, ch in enumerate(self.vocab):\r\n",
        "            self.char_to_idx[ch] = i\r\n",
        "\r\n",
        "    def __call__(self, text):\r\n",
        "        text = text.lower()\r\n",
        "        text = text[: self.max_len - 2]   # later, \"<\" and \">\" will be inserted, hence -2\r\n",
        "        text = \"<\" + text + \">\"\r\n",
        "        pad_len = self.max_len - len(text)\r\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\r\n",
        "\r\n",
        "    def get_vocabulary(self):\r\n",
        "        return self.vocab\r\n",
        "\r\n",
        "\r\n",
        "print(wavs[0]) # Checking if wav path is stored correctly"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\\datasets\\LJSpeech-1.1\\wavs\\LJ001-0001.wav\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLjs5Ay7iA_s",
        "outputId": "1afd5926-20b0-4a6d-e0ab-b6ec1d33fec0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "max_target_len = 200  # all transcripts in out data are < 200 characters\r\n",
        "data = get_data(wavs, id_to_text, max_target_len)\r\n",
        "vectorizer = VectorizeChar(max_target_len)\r\n",
        "print(\"vocab size\", len(vectorizer.get_vocabulary()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 34\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "def create_text_ds(data):\r\n",
        "    texts = [_[\"text\"] for _ in data]\r\n",
        "    text_ds = [vectorizer(t) for t in texts]\r\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\r\n",
        "    return text_ds\r\n",
        "\r\n",
        "\r\n",
        "def path_to_audio(path):\r\n",
        "    # spectrogram using stft\r\n",
        "    audio = tf.io.read_file(path)\r\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\r\n",
        "    audio = tf.squeeze(audio, axis=-1)\r\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\r\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\r\n",
        "    \r\n",
        "    # normalisation\r\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\r\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\r\n",
        "    x = (x - means) / stddevs\r\n",
        "    audio_len = tf.shape(x)[0]\r\n",
        "    \r\n",
        "    # padding to 10 seconds\r\n",
        "    pad_len = 2754\r\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\r\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def create_audio_ds(data):\r\n",
        "    flist = [_[\"audio\"] for _ in data]\r\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\r\n",
        "    audio_ds = audio_ds.map(\r\n",
        "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\r\n",
        "    )\r\n",
        "    return audio_ds\r\n",
        "\r\n",
        "\r\n",
        "def create_tf_dataset(data, bs=4):\r\n",
        "    audio_ds = create_audio_ds(data)\r\n",
        "    text_ds = create_text_ds(data)\r\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\r\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\r\n",
        "    ds = ds.batch(bs)\r\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\r\n",
        "    return ds"
      ],
      "outputs": [],
      "metadata": {
        "id": "tBl4ZLVR4Qul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "SPLIT_RATIO = 0.99\r\n",
        "TRAIN_BATCH_SIZE = 64\r\n",
        "TEST_BATCH_SIZE = 4\r\n",
        "\r\n",
        "split = int(len(data) * SPLIT_RATIO)\r\n",
        "train_data = data[:split]\r\n",
        "test_data = data[split:]\r\n",
        "ds = create_tf_dataset(train_data, bs=TRAIN_BATCH_SIZE)\r\n",
        "valid_ds = create_tf_dataset(test_data, bs=TEST_BATCH_SIZE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wn7_CvLicyUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "print(\"Total train samples: \", len(train_data))\r\n",
        "print(\"Total test samples: \", len(test_data))\r\n",
        "print(\"Shape of each speech feature sample:\", path_to_audio('./datasets/LJSpeech-1.1/wavs/LJ012-0134.wav').shape)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "print(\"5 train samples\")\r\n",
        "print('-'*100)\r\n",
        "for sample in train_data[:5]:\r\n",
        "    print(sample)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "print(\"5 test samples\")\r\n",
        "print('-'*100)\r\n",
        "for sample in test_data[:5]:\r\n",
        "    print(sample)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train samples:  12969\n",
            "Total test samples:  131\n",
            "Shape of each speech feature sample: (2754, 129)\n",
            "\n",
            "\n",
            "5 train samples\n",
            "----------------------------------------------------------------------------------------------------\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ001-0001.wav', 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ001-0002.wav', 'text': 'in being comparatively modern.'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ001-0003.wav', 'text': 'For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ001-0004.wav', 'text': 'produced the block books, which were the immediate predecessors of the true printed book,'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ001-0005.wav', 'text': 'the invention of movable metal letters in the middle of the fifteenth century may justly be considered as the invention of the art of printing.'}\n",
            "\n",
            "\n",
            "5 test samples\n",
            "----------------------------------------------------------------------------------------------------\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ050-0148.wav', 'text': 'the increased information supplied by other agencies will be wasted.'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ050-0149.wav', 'text': 'PRS must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ050-0150.wav', 'text': 'Its present manual filing system is obsolete;'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ050-0151.wav', 'text': 'it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other Government offices.'}\n",
            "{'audio': '.\\\\datasets\\\\LJSpeech-1.1\\\\wavs\\\\LJ050-0152.wav', 'text': 'The Secret Service and the Department of the Treasury now recognize this critical need.'}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "convcQTzdR5G",
        "outputId": "ae5426b8-a77f-4fc8-ba0a-0263f17ccf87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "TmJIhhntfOQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of\n",
        "position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample\n",
        "them (via convolution stides) and process local relationships."
      ],
      "metadata": {
        "id": "xWD9qlHYiA_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "\r\n",
        "class TokenEmbedding(layers.Layer):\r\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\r\n",
        "        super().__init__()\r\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\r\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\r\n",
        "\r\n",
        "    def call(self, x):\r\n",
        "        maxlen = tf.shape(x)[-1]\r\n",
        "        x = self.emb(x)\r\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n",
        "        positions = self.pos_emb(positions)\r\n",
        "        return x + positions\r\n",
        "\r\n",
        "\r\n",
        "class SpeechFeatureEmbedding(layers.Layer):\r\n",
        "    def __init__(self, num_hid=64, maxlen=100):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\r\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\r\n",
        "        )\r\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\r\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\r\n",
        "        )\r\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\r\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\r\n",
        "        )\r\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\r\n",
        "\r\n",
        "    def call(self, x):\r\n",
        "                                # [x] = [batch_size,  pad_len = 2754, num_features = 129] \r\n",
        "        x = self.conv1(x)       # [x] = [batch_size, pad_len = 2754, conv1_num_hid]\r\n",
        "        x = self.conv2(x)       # [x] = [batch_size, pad_len = 2754, conv2_num_hid]\r\n",
        "        x = self.conv3(x)       # [x] = [batch_size, pad_len = 2754, conv3_num_hid]\r\n",
        "        return x\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "8KZWzdQPiA_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "zIOwXzr1iA_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "\r\n",
        "class TransformerEncoder(layers.Layer):\r\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
        "        self.ffn = keras.Sequential(\r\n",
        "            [\r\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\r\n",
        "                layers.Dense(embed_dim),\r\n",
        "            ]\r\n",
        "        )\r\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.dropout1 = layers.Dropout(rate)\r\n",
        "        self.dropout2 = layers.Dropout(rate)\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        attn_output = self.att(inputs, inputs)\r\n",
        "        attn_output = self.dropout1(attn_output, training=training)\r\n",
        "        out1 = self.layernorm1(inputs + attn_output)\r\n",
        "        ffn_output = self.ffn(out1)\r\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\r\n",
        "        return self.layernorm2(out1 + ffn_output)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "RIM1CHz2iA_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "IcdyoUB_iA_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "\r\n",
        "class TransformerDecoder(layers.Layer):\r\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.self_att = layers.MultiHeadAttention(\r\n",
        "            num_heads=num_heads, key_dim=embed_dim\r\n",
        "        )\r\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
        "        self.self_dropout = layers.Dropout(0.5)\r\n",
        "        self.enc_dropout = layers.Dropout(0.1)\r\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\r\n",
        "        self.ffn = keras.Sequential(\r\n",
        "            [\r\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\r\n",
        "                layers.Dense(embed_dim),\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\r\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\r\n",
        "\r\n",
        "        This prevents flow of information from future tokens to current token.\r\n",
        "        1's in the lower triangle, counting from the lower right corner.\r\n",
        "        \"\"\"\r\n",
        "        i = tf.range(n_dest)[:, None]\r\n",
        "        j = tf.range(n_src)\r\n",
        "        m = i >= j - n_src + n_dest\r\n",
        "        mask = tf.cast(m, dtype)\r\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\r\n",
        "        mult = tf.concat(\r\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\r\n",
        "        )\r\n",
        "        return tf.tile(mask, mult)\r\n",
        "\r\n",
        "    def call(self, enc_out, target):\r\n",
        "        input_shape = tf.shape(target)\r\n",
        "        batch_size = input_shape[0]\r\n",
        "        seq_len = input_shape[1]\r\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\r\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\r\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\r\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\r\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\r\n",
        "        ffn_out = self.ffn(enc_out_norm)\r\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\r\n",
        "        return ffn_out_norm\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Oa91CQDRiA_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
        "During training, we give the decoder the target character sequence shifted to the left\n",
        "as input. During inference, the decoder uses its own past predictions to predict the\n",
        "next token."
      ],
      "metadata": {
        "id": "zHAeNDqhiA_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "\r\n",
        "class Transformer(keras.Model):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_hid=64,\r\n",
        "        num_head=2,\r\n",
        "        num_feed_forward=128,\r\n",
        "        source_maxlen=100,\r\n",
        "        target_maxlen=100,\r\n",
        "        num_layers_enc=4,\r\n",
        "        num_layers_dec=1,\r\n",
        "        num_classes=10,\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\r\n",
        "        self.num_layers_enc = num_layers_enc\r\n",
        "        self.num_layers_dec = num_layers_dec\r\n",
        "        self.target_maxlen = target_maxlen\r\n",
        "        self.num_classes = num_classes\r\n",
        "\r\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\r\n",
        "        self.dec_input = TokenEmbedding(\r\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\r\n",
        "        )\r\n",
        "\r\n",
        "        self.encoder = keras.Sequential(\r\n",
        "            [self.enc_input]\r\n",
        "            + [\r\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\r\n",
        "                for _ in range(num_layers_enc)\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "        for i in range(num_layers_dec):\r\n",
        "            setattr(\r\n",
        "                self,\r\n",
        "                f\"dec_layer_{i}\",\r\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\r\n",
        "            )\r\n",
        "\r\n",
        "        self.classifier = layers.Dense(num_classes)\r\n",
        "\r\n",
        "    def decode(self, enc_out, target):\r\n",
        "        y = self.dec_input(target)\r\n",
        "        for i in range(self.num_layers_dec):\r\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        source = inputs[0]\r\n",
        "        target = inputs[1]\r\n",
        "        x = self.encoder(source)\r\n",
        "        y = self.decode(x, target)\r\n",
        "        return self.classifier(y)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def metrics(self):\r\n",
        "        return [self.loss_metric]\r\n",
        "\r\n",
        "    def train_step(self, batch):\r\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\r\n",
        "        source = batch[\"source\"]\r\n",
        "        target = batch[\"target\"]\r\n",
        "        dec_input = target[:, :-1]\r\n",
        "        dec_target = target[:, 1:]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            preds = self([source, dec_input])\r\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\r\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\r\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\r\n",
        "        trainable_vars = self.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "        self.loss_metric.update_state(loss)\r\n",
        "        return {\"loss\": self.loss_metric.result()}\r\n",
        "\r\n",
        "    def test_step(self, batch):\r\n",
        "        source = batch[\"source\"]\r\n",
        "        target = batch[\"target\"]\r\n",
        "        dec_input = target[:, :-1]\r\n",
        "        dec_target = target[:, 1:]\r\n",
        "        preds = self([source, dec_input])\r\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\r\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\r\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\r\n",
        "        self.loss_metric.update_state(loss)\r\n",
        "        return {\"loss\": self.loss_metric.result()}\r\n",
        "\r\n",
        "    def generate(self, source, target_start_token_idx):\r\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\r\n",
        "        bs = tf.shape(source)[0]\r\n",
        "        enc = self.encoder(source)\r\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\r\n",
        "        dec_logits = []\r\n",
        "        for i in range(self.target_maxlen - 1):\r\n",
        "            dec_out = self.decode(enc, dec_input)\r\n",
        "            logits = self.classifier(dec_out)\r\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\r\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\r\n",
        "            dec_logits.append(last_logit)\r\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\r\n",
        "        return dec_input\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "B1v8VtPhiA_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks to display predictions"
      ],
      "metadata": {
        "id": "6PZR074HiA_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "\r\n",
        "class DisplayOutputs(keras.callbacks.Callback):\r\n",
        "    def __init__(\r\n",
        "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\r\n",
        "    ):\r\n",
        "        \"\"\"Displays a batch of outputs after every epoch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\r\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\r\n",
        "            target_start_token_idx: A start token index in the target vocabulary\r\n",
        "            target_end_token_idx: An end token index in the target vocabulary\r\n",
        "        \"\"\"\r\n",
        "        self.batch = batch\r\n",
        "        self.target_start_token_idx = target_start_token_idx\r\n",
        "        self.target_end_token_idx = target_end_token_idx\r\n",
        "        self.idx_to_char = idx_to_token\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        if epoch % 5 != 0:\r\n",
        "            return\r\n",
        "        source = self.batch[\"source\"]\r\n",
        "        target = self.batch[\"target\"].numpy()\r\n",
        "        bs = tf.shape(source)[0]\r\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\r\n",
        "        preds = preds.numpy()\r\n",
        "        for i in range(bs):\r\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\r\n",
        "            prediction = \"\"\r\n",
        "            for idx in preds[i, :]:\r\n",
        "                prediction += self.idx_to_char[idx]\r\n",
        "                if idx == self.target_end_token_idx:\r\n",
        "                    break\r\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\r\n",
        "            print(f\"prediction: {prediction}\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "uRCgKWmViA_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate schedule"
      ],
      "metadata": {
        "id": "yK-HUISpiA_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "\r\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        init_lr=0.00001,\r\n",
        "        lr_after_warmup=0.001,\r\n",
        "        final_lr=0.00001,\r\n",
        "        warmup_epochs=15,\r\n",
        "        decay_epochs=85,\r\n",
        "        steps_per_epoch=203,\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.init_lr = init_lr\r\n",
        "        self.lr_after_warmup = lr_after_warmup\r\n",
        "        self.final_lr = final_lr\r\n",
        "        self.warmup_epochs = warmup_epochs\r\n",
        "        self.decay_epochs = decay_epochs\r\n",
        "        self.steps_per_epoch = steps_per_epoch\r\n",
        "\r\n",
        "    def calculate_lr(self, epoch):\r\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\r\n",
        "        warmup_lr = (\r\n",
        "            self.init_lr\r\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\r\n",
        "        )\r\n",
        "        decay_lr = tf.math.maximum(\r\n",
        "            self.final_lr,\r\n",
        "            self.lr_after_warmup\r\n",
        "            - (epoch - self.warmup_epochs)\r\n",
        "            * (self.lr_after_warmup - self.final_lr)\r\n",
        "            / (self.decay_epochs),\r\n",
        "        )\r\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\r\n",
        "\r\n",
        "    def __call__(self, step):\r\n",
        "        epoch = step // self.steps_per_epoch\r\n",
        "        return self.calculate_lr(epoch)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "B9kkxdqbiA_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create & train the end-to-end model"
      ],
      "metadata": {
        "id": "xy1VairWiA_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "NUM_EPOCH = 100\r\n",
        "\r\n",
        "batch = next(iter(valid_ds))\r\n",
        "\r\n",
        "# The vocabulary to convert predicted indices into characters\r\n",
        "idx_to_char = vectorizer.get_vocabulary()\r\n",
        "display_cb = DisplayOutputs(\r\n",
        "    batch, idx_to_char,  target_start_token_idx=2, target_end_token_idx=3\r\n",
        ")  # set the arguments as per vocabulary index for '<' and '>'\r\n",
        "\r\n",
        "model = Transformer(\r\n",
        "    num_hid=200,\r\n",
        "    num_head=2,\r\n",
        "    num_feed_forward=400,\r\n",
        "    target_maxlen=max_target_len,\r\n",
        "    num_layers_enc=4,\r\n",
        "    num_layers_dec=5,\r\n",
        "    num_classes=34,\r\n",
        ")\r\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\r\n",
        "    from_logits=True, label_smoothing=0.1,\r\n",
        ")\r\n",
        "\r\n",
        "learning_rate = CustomSchedule(\r\n",
        "    init_lr=0.00001,\r\n",
        "    lr_after_warmup=0.001,\r\n",
        "    final_lr=0.00001,\r\n",
        "    warmup_epochs=15,\r\n",
        "    decay_epochs=85,\r\n",
        "    steps_per_epoch=len(ds),\r\n",
        ")\r\n",
        "\r\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\r\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\r\n",
        "\r\n",
        "history = model.fit(ds, validation_data=valid_ds, callbacks=[display_cb], epochs=NUM_EPOCH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "203/203 [==============================] - 154s 706ms/step - loss: 1.5605 - val_loss: 1.5467\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the the ar the the t athe the the te a oo the te as an t the the t t t t t or the the the t o the the t the othe t the t ale atere the the t alas te o t tore the are t t the the the t s the te te t a\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <the the t the athe t athe the the te a oo the te as the t the t the t an t o t t the the the the t the an the the o s the t o ase athe the t alas te o t tore the t t t t the the the t s the t a te o \n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <the the ar the the t athe the the te a oo the te as an t the the t on an t or the the the t o the the t the othe t the t ale atere the the t alas te o t tore the are t t the the the t s the te te t a\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <the the t the athe t athe the the te a oo the te as the t the t the t an t o t t the the the the t the an the the are the t o ase athe the t alas te o t to the o t t t t the the the t s the t a te o \n",
            "\n",
            "Epoch 2/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 1.3506 - val_loss: 1.4063\n",
            "Epoch 3/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 1.2988 - val_loss: 1.3339\n",
            "Epoch 4/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 1.2081 - val_loss: 1.2193\n",
            "Epoch 5/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 1.1330 - val_loss: 1.1333\n",
            "Epoch 6/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 1.0765 - val_loss: 1.0637\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the secret service and secret the secret fived and the secret for the presidenty.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <the secret service as and secret asssassing and secret and the president of the secret and the secret of the sective and the sective of the secticed.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <the secret servied the secret service of the secret service.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <the secret of the secret and the presidence and and and and and and and and the secret and the secret of the secret and the secret of the president.>\n",
            "\n",
            "Epoch 7/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 1.0272 - val_loss: 1.0074\n",
            "Epoch 8/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.9805 - val_loss: 0.9496\n",
            "Epoch 9/100\n",
            "203/203 [==============================] - 141s 695ms/step - loss: 0.9173 - val_loss: 0.8672\n",
            "Epoch 10/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.8277 - val_loss: 0.7740\n",
            "Epoch 11/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.7418 - val_loss: 0.7061\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the encret stepparations while been creasiderations while been creased.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <president as in the president basis in the president basis of devisite of deprats the classificient basising to classing the president bepressed of depressiden.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <it sould in mannelly the manual file five manual filey is one is one is office of the man sumet.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it mattic and in the bising which devising of the recenting office in the bising which and in other which devised devising office and in other which deveral express.>\n",
            "\n",
            "Epoch 12/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.6810 - val_loss: 0.6524\n",
            "Epoch 13/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.6360 - val_loss: 0.6229\n",
            "Epoch 14/100\n",
            "203/203 [==============================] - 142s 697ms/step - loss: 0.6037 - val_loss: 0.6068\n",
            "Epoch 15/100\n",
            "203/203 [==============================] - 140s 690ms/step - loss: 0.5788 - val_loss: 0.5885\n",
            "Epoch 16/100\n",
            "203/203 [==============================] - 140s 690ms/step - loss: 0.5575 - val_loss: 0.5748\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supply was roomation supply was roomation supply was roomation.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <present that could has in juied rate basist on a more suffic basist on amoraphic basist on a more suffic basit basis to class of devicted basity to classed to classity.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its presently the beling self sleed solf filed solfial filed sistement.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make make authorite office authorities which are which are which are which are which are which are which are which are which are which are which are white daticencenceds.>\n",
            "\n",
            "Epoch 17/100\n",
            "203/203 [==============================] - 141s 691ms/step - loss: 0.5377 - val_loss: 0.5664\n",
            "Epoch 18/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.5213 - val_loss: 0.5558\n",
            "Epoch 19/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.5076 - val_loss: 0.5464\n",
            "Epoch 20/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.4958 - val_loss: 0.5367\n",
            "Epoch 21/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4850 - val_loss: 0.5377\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supply mation sted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <present judge a great breake develop the class of jue on more subjects on more son more subjects on more suffices than the present juices than.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual file of sonstant manual fileing system is on sliet.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make make developments in the busines developments in the busen office in other covelusing which are which a recent date of the recent date office.>\n",
            "\n",
            "Epoch 22/100\n",
            "203/203 [==============================] - 141s 695ms/step - loss: 0.4784 - val_loss: 0.5291\n",
            "Epoch 23/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4682 - val_loss: 0.5265\n",
            "Epoch 24/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4592 - val_loss: 0.5211\n",
            "Epoch 25/100\n",
            "203/203 [==============================] - 141s 695ms/step - loss: 0.4535 - val_loss: 0.5179\n",
            "Epoch 26/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.4474 - val_loss: 0.5109\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supply by other agested.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <president juade subjects on a more suffic president jua grassity at subjects a more subjects must to classity to classity to closity to classity.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual file sonsoling system is onsulfiling sysistem is onswling sysongelt.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes in the business were which are whidely use and in other which are whidely use at it makes no use of the resoness in the business in other covernment officdence.>\n",
            "\n",
            "Epoch 27/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4378 - val_loss: 0.5080\n",
            "Epoch 28/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.4308 - val_loss: 0.5025\n",
            "Epoch 29/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.4250 - val_loss: 0.5005\n",
            "Epoch 30/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.4228 - val_loss: 0.4976\n",
            "Epoch 31/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4176 - val_loss: 0.4940\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supply by by other agencies will be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs tho judgraphic develop the copy are assity to class on a more suffic develop the capacity of has and the president jeaute subjects on a moresific rasity to these to theseassed.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its presency on is onsoling system is onswill file file system estem estem esteme esteme este.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes of the resendenth and in the business were altments in the business of the resendent office and in other government office in the below use and oftements.>\n",
            "\n",
            "Epoch 32/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4127 - val_loss: 0.4909\n",
            "Epoch 33/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.4127 - val_loss: 0.4904\n",
            "Epoch 34/100\n",
            "203/203 [==============================] - 140s 690ms/step - loss: 0.4073 - val_loss: 0.4891\n",
            "Epoch 35/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4024 - val_loss: 0.4861\n",
            "Epoch 36/100\n",
            "203/203 [==============================] - 141s 691ms/step - loss: 0.3989 - val_loss: 0.4798\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased in formation supplied by by other agested.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs evelop the could has and jure great dan#t the present june of has and the present june agreat braffic reson a more suffic breaken to clasity to clasity to clasity thane asitinged ty the the t tou\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual file in system is obsiling system is onsistam is to manual filencest im isonsistim ent.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes no use of the resend in other governments in the business were awfice and in office of the resend officent in the busing which are white date prossessing and in othelermancicent ouand ond on\n",
            "\n",
            "Epoch 37/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.4014 - val_loss: 0.4893\n",
            "Epoch 38/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.4087 - val_loss: 0.4799\n",
            "Epoch 39/100\n",
            "203/203 [==============================] - 140s 688ms/step - loss: 0.3958 - val_loss: 0.4839\n",
            "Epoch 40/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.3908 - val_loss: 0.4788\n",
            "Epoch 41/100\n",
            "203/203 [==============================] - 140s 688ms/step - loss: 0.4100 - val_loss: 0.5514\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <these information supplied by agencies worl be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs a fictic assignatic rassity to classity to classity to classity to classity to classity agraphic raffic raphic raffic raffic refic rassity.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its presence to manual filing sysile exist imanual filingst.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes mover of the resenant in the businges of the resencent in office or which are guald, which a gergular governments inelficences.>\n",
            "\n",
            "Epoch 42/100\n",
            "203/203 [==============================] - 140s 689ms/step - loss: 0.4612 - val_loss: 0.4767\n",
            "Epoch 43/100\n",
            "203/203 [==============================] - 140s 691ms/step - loss: 0.4047 - val_loss: 0.4692\n",
            "Epoch 44/100\n",
            "203/203 [==============================] - 140s 687ms/step - loss: 0.3929 - val_loss: 0.4728\n",
            "Epoch 45/100\n",
            "203/203 [==============================] - 140s 689ms/step - loss: 0.3857 - val_loss: 0.4693\n",
            "Epoch 46/100\n",
            "203/203 [==============================] - 140s 689ms/step - loss: 0.3832 - val_loss: 0.4692\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supplie by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs it subject the capacity to class a fices than the present j. b. are affic develop the capacity agraphic divide subjects on a more suffict resside.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <it s present manual filence to is obsile existem is obsistem is tome ent.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes not use of the recent in the business were ald which a recent office of the recent office in other government#s in the business of the regovernments in the bused sands.>\n",
            "\n",
            "Epoch 47/100\n",
            "203/203 [==============================] - 143s 702ms/step - loss: 0.3776 - val_loss: 0.4720\n",
            "Epoch 48/100\n",
            "203/203 [==============================] - 141s 691ms/step - loss: 0.3744 - val_loss: 0.4701\n",
            "Epoch 49/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.3707 - val_loss: 0.4704\n",
            "Epoch 50/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3685 - val_loss: 0.4712\n",
            "Epoch 51/100\n",
            "203/203 [==============================] - 141s 694ms/step - loss: 0.3673 - val_loss: 0.4744\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supplied by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs it subjects on a more suffices than the president the capacity of great develop the catest to classity to classity to the are affic raphic beasis jecade beatne.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual fileing sistem is obsolete>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes no use of the recent developments in the business were altments in other government of the recent of processing which are widely use of the reasoned evelopments inther governmenments.>\n",
            "\n",
            "Epoch 52/100\n",
            "203/203 [==============================] - 142s 698ms/step - loss: 0.3655 - val_loss: 0.4694\n",
            "Epoch 53/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3651 - val_loss: 0.4718\n",
            "Epoch 54/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3633 - val_loss: 0.4675\n",
            "Epoch 55/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.3599 - val_loss: 0.4722\n",
            "Epoch 56/100\n",
            "203/203 [==============================] - 141s 691ms/step - loss: 0.3592 - val_loss: 0.4804\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supplied by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs in juicapated basis than the present ju graphic brack dan# a more sufficient to classity to geographic brake gat than the presidenticated the than a more subjee asmifices musticality j justocicec\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual filences to manual filences obsolite>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes of the recent developments in developments in the busing which are resent developments in office of the regovernments in the business were all imaticdatopratice.>\n",
            "\n",
            "Epoch 57/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3587 - val_loss: 0.4732\n",
            "Epoch 58/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.3580 - val_loss: 0.4701\n",
            "Epoch 59/100\n",
            "203/203 [==============================] - 141s 692ms/step - loss: 0.3551 - val_loss: 0.4738\n",
            "Epoch 60/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3549 - val_loss: 0.4738\n",
            "Epoch 61/100\n",
            "203/203 [==============================] - 141s 691ms/step - loss: 0.3557 - val_loss: 0.4707\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information cesting supplied by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs if as a jeet subjects on a more suffic pracity to classity of great develop the capated basis than the present joud raffic present jects on more subjects must tochessity,>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual fileing system as obsolute,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make snow use and in the business were alded which are which are recent in office of the reason developments in other governments in other guvernment office.>\n",
            "\n",
            "Epoch 62/100\n",
            "203/203 [==============================] - 141s 695ms/step - loss: 0.3557 - val_loss: 0.4733\n",
            "Epoch 63/100\n",
            "203/203 [==============================] - 141s 693ms/step - loss: 0.3529 - val_loss: 0.4737\n",
            "Epoch 64/100\n",
            "203/203 [==============================] - 146s 716ms/step - loss: 0.3524 - val_loss: 0.4731\n",
            "Epoch 65/100\n",
            "203/203 [==============================] - 140s 687ms/step - loss: 0.3508 - val_loss: 0.4729\n",
            "Epoch 66/100\n",
            "203/203 [==============================] - 140s 687ms/step - loss: 0.3482 - val_loss: 0.4762\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information ceupt information supplied by other agencies will be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs in geographic present geographic develop the capacity to classify are at sufficient jown more subject on a more sufficient to classity, to than.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual fileing system is obsistem is obsistem esceling system #>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes of the recent developments in development office in other government in the business no use of the recent developments in office of the regvernment of the usa.>\n",
            "\n",
            "Epoch 67/100\n",
            "203/203 [==============================] - 140s 686ms/step - loss: 0.3475 - val_loss: 0.4750\n",
            "Epoch 68/100\n",
            "203/203 [==============================] - 139s 686ms/step - loss: 0.3469 - val_loss: 0.4734\n",
            "Epoch 69/100\n",
            "203/203 [==============================] - 140s 689ms/step - loss: 0.3500 - val_loss: 0.4739\n",
            "Epoch 70/100\n",
            "203/203 [==============================] - 139s 686ms/step - loss: 0.3452 - val_loss: 0.4741\n",
            "Epoch 71/100\n",
            "203/203 [==============================] - 139s 686ms/step - loss: 0.3439 - val_loss: 0.4760\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supplie by other agencies will be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs offic ress must to classific develop the cated basis than the present journe than the present jraphic develop the capacity to classity, to cllassity, to jown.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual fileing system is obsistam is oxim is to manual exist im manual filengesist,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make spore which are recent in the business of the recent office world, and in other governments in office of the use of the recent in the business.>\n",
            "\n",
            "Epoch 72/100\n",
            "203/203 [==============================] - 139s 686ms/step - loss: 0.3431 - val_loss: 0.4746\n",
            "Epoch 73/100\n",
            "203/203 [==============================] - 140s 687ms/step - loss: 0.3424 - val_loss: 0.4796\n",
            "Epoch 74/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3421 - val_loss: 0.4795\n",
            "Epoch 75/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3412 - val_loss: 0.4792\n",
            "Epoch 76/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3409 - val_loss: 0.4811\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information cecuplied by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <p.m., rack at asked to classify at subject than the present juice than the present jee of raphic reked and a more suffice must evelop the cated basisis to jown.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual fileing system is obsolute,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it makes notice of the recent the business were ald, which are white data office and in other governments in the business of the use of the recent of process.>\n",
            "\n",
            "Epoch 77/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3399 - val_loss: 0.4770\n",
            "Epoch 78/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3405 - val_loss: 0.4764\n",
            "Epoch 79/100\n",
            "203/203 [==============================] - 139s 684ms/step - loss: 0.3396 - val_loss: 0.4749\n",
            "Epoch 80/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3387 - val_loss: 0.4731\n",
            "Epoch 81/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3380 - val_loss: 0.4765\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information ciplie by other agencies will be wastent.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <prs a fice must ecated be adgraphic requitant subjects on a more suffice than the present geographic develop the cated to classity to classity to classity are assmusty,>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <it spresent manual file of file the fileing system is obsistem escelling system is obsolized,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make snow use of the recent developments in other government office on office were alter which are which are recent in the business were old, white dataitons.>\n",
            "\n",
            "Epoch 82/100\n",
            "203/203 [==============================] - 139s 686ms/step - loss: 0.3376 - val_loss: 0.4762\n",
            "Epoch 83/100\n",
            "203/203 [==============================] - 139s 684ms/step - loss: 0.3372 - val_loss: 0.4799\n",
            "Epoch 84/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3368 - val_loss: 0.4778\n",
            "Epoch 85/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3363 - val_loss: 0.4743\n",
            "Epoch 86/100\n",
            "203/203 [==============================] - 139s 684ms/step - loss: 0.3360 - val_loss: 0.4734\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information supplied by other agencies will be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <p. are at smust evelop the capacity to classific resen geographic at has a jeer great develop the cated basis than the present jurgect on amore suffice to juckettam.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual file exhist im is obsoliet.>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make spessisan developments in office of the recent of processing which are white data #and in the business of the used in other governments in the busing used in otomatte icdata oppr preacelicthe\n",
            "\n",
            "Epoch 87/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3356 - val_loss: 0.4722\n",
            "Epoch 88/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3351 - val_loss: 0.4760\n",
            "Epoch 89/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3348 - val_loss: 0.4761\n",
            "Epoch 90/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3346 - val_loss: 0.4758\n",
            "Epoch 91/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3343 - val_loss: 0.4757\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information ceased in from ation supplied by other agenirge, wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <p.s. jewelophoced to classify attocity to classify are affic rachic resen geated basis than the present judgect and the present the presity to jeck game.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual filee exhist im is obsoliet,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make sno use of the recent developments in the business were ald, which a wide leason developments of the usued in other government in the business.>\n",
            "\n",
            "Epoch 92/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3341 - val_loss: 0.4744\n",
            "Epoch 93/100\n",
            "203/203 [==============================] - 139s 684ms/step - loss: 0.3340 - val_loss: 0.4748\n",
            "Epoch 94/100\n",
            "203/203 [==============================] - 139s 685ms/step - loss: 0.3339 - val_loss: 0.4756\n",
            "Epoch 95/100\n",
            "203/203 [==============================] - 139s 682ms/step - loss: 0.3338 - val_loss: 0.4764\n",
            "Epoch 96/100\n",
            "203/203 [==============================] - 139s 682ms/step - loss: 0.3338 - val_loss: 0.4767\n",
            "target:     <the increased information supplied by other agencies will be wasted.>\n",
            "prediction: <the increased information ceased in supplied by other agencies will be wasted.>\n",
            "\n",
            "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
            "prediction: <p. are affic racked to classify atsisticated basis than the present geographic brachadson a more suffice residea to classity to clar at smust to jee of has must to juckecked am.>\n",
            "\n",
            "target:     <its present manual filing system is obsolete#>\n",
            "prediction: <its present manual filee exhileing system is obsoliet,>\n",
            "\n",
            "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
            "prediction: <it make sno usunal ments in the business of the recent office of the usur governments in other government of the usand office were all develous #and in the busing, which are white deeperather peather\n",
            "\n",
            "Epoch 97/100\n",
            "203/203 [==============================] - 139s 683ms/step - loss: 0.3337 - val_loss: 0.4770\n",
            "Epoch 98/100\n",
            "203/203 [==============================] - 139s 684ms/step - loss: 0.3337 - val_loss: 0.4783\n",
            "Epoch 99/100\n",
            "203/203 [==============================] - 1001s 5s/step - loss: 0.3337 - val_loss: 0.4786\n",
            "Epoch 100/100\n",
            "203/203 [==============================] - 921s 5s/step - loss: 0.3336 - val_loss: 0.4791\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTBkxxe7iA_u",
        "outputId": "f93793d8-ce2f-4669-9b08-daa57d40f23d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "model.summary(line_length=110)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "______________________________________________________________________________________________________________\n",
            "Layer (type)                                     Output Shape                                Param #          \n",
            "==============================================================================================================\n",
            "speech_feature_embedding (SpeechFeatureEmbedding (None, None, 200)                           1164400          \n",
            "______________________________________________________________________________________________________________\n",
            "token_embedding (TokenEmbedding)                 multiple                                    46800            \n",
            "______________________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)                        (None, None, 200)                           3095600          \n",
            "______________________________________________________________________________________________________________\n",
            "transformer_decoder (TransformerDecoder)         multiple                                    804600           \n",
            "______________________________________________________________________________________________________________\n",
            "transformer_decoder_1 (TransformerDecoder)       multiple                                    804600           \n",
            "______________________________________________________________________________________________________________\n",
            "transformer_decoder_2 (TransformerDecoder)       multiple                                    804600           \n",
            "______________________________________________________________________________________________________________\n",
            "transformer_decoder_3 (TransformerDecoder)       multiple                                    804600           \n",
            "______________________________________________________________________________________________________________\n",
            "transformer_decoder_4 (TransformerDecoder)       multiple                                    804600           \n",
            "______________________________________________________________________________________________________________\n",
            "dense_18 (Dense)                                 multiple                                    6834             \n",
            "==============================================================================================================\n",
            "Total params: 7,172,236\n",
            "Trainable params: 7,172,234\n",
            "Non-trainable params: 2\n",
            "______________________________________________________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "_xNWWcXn_vyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57de519-d56e-4340-928f-d663fa20ff51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating results on validation dataset\r\n"
      ],
      "metadata": {
        "id": "yzpGA_wKCaI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "predictions = []\r\n",
        "targets = []\r\n",
        "for batch in valid_ds:\r\n",
        "    source = batch[\"source\"]\r\n",
        "    target = batch[\"target\"].numpy()\r\n",
        "    bs = tf.shape(source)[0]\r\n",
        "    preds = model.generate(source, display_cb.target_start_token_idx).numpy()\r\n",
        "    for i in range(bs):\r\n",
        "            target_text = \"\".join([display_cb.idx_to_char[_] for _ in target[i, :]])\r\n",
        "            prediction = \"\"\r\n",
        "            for idx in preds[i, :]:\r\n",
        "                prediction += display_cb.idx_to_char[idx]\r\n",
        "                if idx == display_cb.target_end_token_idx:\r\n",
        "                    break\r\n",
        "            targets.append(target_text.replace('-',''))\r\n",
        "            predictions.append(prediction)\r\n",
        "\r\n",
        "wer_score = wer(targets, predictions)\r\n",
        "print(\"-\" * 100)\r\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\r\n",
        "print(\"-\" * 100)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 0.6791\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "metadata": {
        "id": "8Q6uAM2b_CTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca5b7ec-8e84-49c9-d677-26dad10f6c84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "for i in np.random.randint(0, len(predictions), 5):\r\n",
        "    print(f\"Target    : {targets[i]}\")\r\n",
        "    print(f\"Prediction: {predictions[i]}\")\r\n",
        "    print(\"-\" * 100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target    : <secret service personnel and facilities>\n",
            "Prediction: <secret service personal and facilities.>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : <to make certain that all protective intelligence activities are coordinated.>\n",
            "Prediction: <to make sertain that all protective a naties are coordinated.>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : <while these statistics relate to the activities of secret service agents stationed in field offices and not the white house detail,>\n",
            "Prediction: <while these to tistics relate house detail, stationed in field offices agents secret service and not awite house detail,>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : <however, the nineteen sixtyfour to sixtyfive budget request was submitted in november nineteen sixtythree>\n",
            "Prediction: <however, it the nineteen sixty for quest was submitted in november nineteen sixty five bugest three>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : <to assist in its protection functions.>\n",
            "Prediction: <to assist in its protections.>\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5AiY9yQJFwB",
        "outputId": "22dc4d79-b530-43e8-e721-e094f89a873e"
      }
    }
  ]
}